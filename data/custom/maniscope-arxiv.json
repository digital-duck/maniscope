[
  {
    "query": "",
    "docs": [
      "## Optimizing RAG Reranker via Geodesic Distances on k-NN Manifolds\n\nWen G. Gong ∗\n\nJanuary 24, 2026",
      "## Abstract\n\nCurrent neural reranking approaches rely on cross-encoders or large language models, requiring substantial computational resources (4-15 seconds per query). We propose Maniscope, a geometric reranking method that computes geodesic distances on k-nearest neighbor manifolds constructed over retrieved candidates. Through algorithmic optimization, we dramatically improve reranking efficiency while preserving quality. This approach combines global cosine similarity with local manifold geometry to capture semantic structure that flat Euclidean distance metrics miss.\n\nOn eight BEIR-format benchmark datasets (1,233 queries total across SciFact, MS MARCO, TREC-COVID, ArguAna, FiQA, NFCorpus, FEVER, and a novel disambiguation dataset AorB), Maniscope achieves competitive accuracy (average MRR: 0.9691, NDCG@3: 0.9199) while maintaining consistent sub-5ms latency. Maniscope outperforms the HNSW graph-based baseline on the three hardest datasets (NFCorpus: +7.0%, TREC-COVID: +1.6%, AorB: +2.8% NDCG@3) while being 3.2 × faster (4.7ms vs 14.8ms average). Compared to cross-encoder rerankers, Maniscope achieves within 2% accuracy at 10-45 × lower latency. The method requires O ( ND ) for initial retrieval plus O ( M 2 D + Mk log k ) for reranking over M candidates where M ≪ N , making real-time RAG deployment practical. We plan to release Maniscope as open-source software.\n\nKeywords: Information Retrieval, Reranking, Manifold Learning, Geodesic Distance, Neural Search, RAG, Algorithmic Optimization",
      "## 1.1 Motivation\n\nRetrieval-augmented generation (RAG) has emerged as a critical paradigm for enhancing large language models with external knowledge [1]. The quality of RAG systems fundamentally depends on retrieving the most relevant documents from vast corpora. Current state-of-the-art approaches rely almost exclusively on cosine similarity in dense embedding spaces produced by models like BERT [2], Sentence-BERT [3], or specialized retrieval models [4].\n\nWhile cosine similarity is computationally efficient and provides reasonable global rankings, it treats the embedding space as flat Euclidean. This ignores local structure in learned semantic representations. Embeddings form clusters and neighborhoods that reflect semantic relationships [5, 6]. Documents that are globally distant may be semantically similar within local neighborhoods.\n\nAs an example, we consider the query 'What does a python eat?' . In embedding space, documents about python programming (syntax, packages, exceptions) form one cluster, while documents about python snakes (diet, habitat, venom) form another cluster. Cosine similarity measures the global angle between query and documents, potentially ranking a highly relevant document about 'Python programming basics' over the correct 'python snake diet' if the query vector happens to be slightly closer to the programming cluster. However, within the local manifold of animal-related documents, the correct answer is much closer along geodesic paths.\n\n∗ wen.gong.research@gmail.com",
      "## 1.2 Our Contribution\n\nWe propose Geodesic Reranking, a two-stage retrieval method. The first stage (telescope) performs broad initial retrieval using cosine similarity to identify candidates. The second stage (microscope) reranks using geodesic distances on a k-nearest neighbor (k-NN) manifold graph constructed over candidates.\n\nThe method constructs a k-NN manifold graph over retrieved candidates and computes geodesic distances at query time to leverage local structure.\n\nOur contributions include: (1) applying geodesic distances on k-NN manifolds for reranking, (2) hybrid scoring combining global cosine similarity with local geodesic similarity via α -weighting, (3) implementation with O ( ND + M 2 D + Mk log k ) complexity where M ≪ N , (4) algorithmic optimization achieving sub-10ms latency through versions v0 → v1 → v2 → v2o, (5) a novel disambiguation benchmark dataset (AorB) for evaluating semantic disambiguation, (6) evaluation on eight BEIR-format datasets (1,233 queries) showing Maniscope outperforms HNSW graph-based baseline on three hardest datasets while being 2 × faster, and achieves competitive accuracy with cross-encoders at 6-29 × lower latency, (7) open-source toolkit with evaluation framework and API.",
      "## 1.3 Paper Organization\n\nSection 2 reviews related work in retrieval, reranking, and manifold learning. Section 3 presents our Geodesic Reranking method with detailed algorithms. Section 4 describes our experimental setup, datasets, and baselines. Section 5 presents evaluation results on eight BEIR-format datasets including a novel disambiguation benchmark, comparing Maniscope against HNSW graph-based baseline and cross-encoder rerankers. Section 6 analyzes when and why geodesic reranking helps. Section 7 discusses current limitations and future research directions. Section 8 concludes with discussion of implications for algorithmic efficiency and environmental sustainability.",
      "## 2.1 Dense Retrieval and RAG\n\nDense retrieval methods [4, 7] have largely replaced sparse methods (BM25, TF-IDF) for semantic search. Modern approaches use dual-encoder architectures [3] to embed queries and documents into shared spaces, ranking by cosine similarity or dot product. Retrieval-augmented generation (RAG) [1, 8] extends this by providing retrieved contexts to language models, substantially improving factual accuracy and reducing hallucinations.\n\nHowever, these methods treat embedding spaces as flat Euclidean manifolds, ignoring learned geometric structure. Our work addresses this limitation by incorporating local manifold topology.",
      "## 2.2 Reranking Methods\n\nCross-encoders [9] jointly encode query-document pairs for more accurate ranking but are computationally expensive with O ( N ) forward passes per query. ColBERT [10] performs late interaction between token embeddings, achieving strong performance with reduced cost. Listwise reranking [11] optimizes entire result lists rather than pairwise scores.\n\nOur approach is complementary: we rerank based on geometric structure rather than neural scoring, offering computational efficiency and interpretability.",
      "## 2.3 Manifold Learning\n\nManifold learning methods [12, 13, 14, 15] uncover low-dimensional structure in high-dimensional data. Isomap [12] explicitly uses geodesic distances on k-NN graphs for dimensionality reduction, while PHATE [16] employs potential distances on diffusion graphs for visualization.\n\nWhile these methods focus on visualization or feature learning, we apply similar principles directly to retrieval ranking, preserving the original embedding space while leveraging local manifold structure.",
      "## 2.4 Graph-Based Retrieval\n\nPageRank [17] and random walk methods [18] have been applied to document ranking. Recent work explores neural graph models [19, 20] for knowledge graph reasoning. HNSW [21] builds hierarchical navigable small-world graphs for approximate nearest neighbor search.\n\nOur k-NN graph serves a different purpose: rather than approximate search, we use it to compute exact geodesic distances for reranking, capturing semantic neighborhoods that cosine similarity misses.",
      "## 3.1 Problem Formulation\n\nGiven:\n\n- A document corpus D = { d 1 , . . . , d N }\n- An embedding function ϕ : D → R D (e.g., Sentence-BERT)\n- A query q with embedding ϕ ( q )\n\nGoal: Retrieve topk most relevant documents from D .",
      "## 3.2 Baseline: Cosine Similarity (Telescope)\n\nStandard dense retrieval ranks documents by cosine similarity:\n\n<!-- formula-not-decoded -->\n\nThis provides a global ranking: rank( d i ) = arg max j s cos ( q, d j ). However, cosine similarity measures angular distance in the full embedding space, treating it as flat Euclidean. It may miss local manifold structure where semantically similar documents cluster together.",
      "## 3.3 Geodesic Reranking (Maniscope)\n\nOur method enhances cosine similarity with local geodesic distances on a k-NN manifold graph constructed over retrieved candidates.",
      "## 3.3.1 Query-Time Processing\n\n1. Coarse retrieval\n\n<!-- formula-not-decoded -->\n\nRetrieve topM candidates ( M ≫ k , e.g., M = 3 k ) using cosine similarity.\n\n2. Geodesic reranking\n2. (a) Build k-NN graph over candidates: Construct G C = ( C, E, w ) where vertices are the M retrieved candidates C , edges connect each document to its k-nearest neighbors within C , and weights are defined as w ( d i , d j ) = 1 -s cos ( d i , d j ).\n3. (b) Select anchor node : d anchor = arg max d i ∈ C s cos ( q, d i )\n4. (c) Compute geodesic distances: ∀ d i ∈ C ,\n\n<!-- formula-not-decoded -->\n\nusing Dijkstra's algorithm on G C .\n\n- (d) Convert to similarity score:\n\n<!-- formula-not-decoded -->\n\n3. Hybrid scoring\n\n<!-- formula-not-decoded -->\n\nwhere α ∈ [0 , 1] balances global (cosine) vs. local (geodesic) similarity.\n\n4. Return topk documents by s final .\n\nComplexity: Step-1 requires O ( ND ) for computing cosine similarities over the corpus. Building the candidate graph requires O ( M 2 D ) for computing pairwise similarities among M candidates. Step-2 requires O ( Mk log k ) for Dijkstra on the candidate graph. The total per-query complexity is O ( ND + M 2 D + Mk log k ) = O ( ND + M 2 D ) since M ≪ N . In practice, M is small (e.g., M = 10 -100), making the O ( M 2 D ) graph construction negligible compared to the initial O ( ND ) retrieval.",
      "## 3.4 Design Rationale\n\n- Why build the graph over candidates only? Building a k-NN graph over the entire corpus ( N documents) would require O ( N 2 D ) computation, which is prohibitive for large corpora. By constructing the graph only over the topM retrieved candidates where M ≪ N , we reduce this to O ( M 2 D ), making it practical for real-time reranking.\n- Why k-NN graph? Captures local neighborhoods among retrieved candidates without requiring global pairwise distances.\n- Why geodesic distance? Respects manifold geometry: shortest paths traverse semantically coherent neighborhoods.\n- Why anchor node? Provides a reference point within the candidate set. The anchor is the 'most query-relevant' document; geodesic distances from it measure relative relevance within local context.\n- Why hybrid scoring ( α )? Balances global (telescope) and local (microscope) information. Pure geodesic ( α = 0) loses global context; pure cosine ( α = 1) ignores local structure. Empirically, α ≈ 0 . 5 works well.\n- Handling disconnected components: If d i is unreachable from d anchor (different cluster), set δ geo = ∞ , so s geo = 0. The final score reverts to cosine similarity, providing a graceful fallback.",
      "## 3.5 Theoretical Justification\n\nGeodesic distance on k-NN graphs approximates the intrinsic manifold distance when embeddings lie on a low-dimensional manifold [12]. By reranking with geodesic scores, we effectively measure:\n\n<!-- formula-not-decoded -->\n\nrather than Euclidean/angular distance in the ambient embedding space.\n\nThis aligns with the manifold hypothesis : high-dimensional data (embeddings) often lie on lowerdimensional manifolds [22], and meaningful distances should respect this geometry.",
      "## 4.1 Datasets\n\nWe evaluate on eight diverse datasets (1,233 queries total) to assess generalization across different retrieval tasks and domains. Seven are established BEIR benchmarks [23], plus one new disambiguation benchmark:\n\n- SciFact [24]: Scientific claim verification (300 docs, 100 queries). Tests fine-grained semantic distinction between supporting and refuting evidence in scientific literature.\n\n- MS MARCO [25]: Web search passages (8.8M passages, 200 queries). Evaluates performance on short, diverse web queries with varied intents.\n- TREC-COVID [26]: COVID-19 research articles (171,332 docs, 50 queries). Evaluates performance on technical medical queries with precise terminology.\n- ArguAna : Counter-argument retrieval (8,674 docs, 100 queries). Tests ability to capture semantic nuances in argumentative structures.\n- FiQA : Financial question answering (57,638 docs, 100 queries). Assesses robustness to domainspecific terminology and jargon.\n- NFCorpus : Medical/nutrition information retrieval (3,633 docs, 323 queries). Challenging dataset with domain-specific terminology requiring strong semantic understanding.\n- FEVER : Fact verification (5,416 docs, 200 queries). Tests retrieval of evidence for fact-checking claims.\n- AorB (new): Semantic disambiguation benchmark (200 docs, 50 queries). Tests retrieval on ambiguous terms with dual meanings (e.g., 'Python' as snake vs. programming language). See Appendix A for dataset details.\n\nThese datasets span diverse domains (scientific, medical, financial, web search, argumentation, fact verification, disambiguation), query types (factual, semantic, technical, ambiguous), and corpus sizes (200 to 8.8M documents), providing comprehensive evaluation of geodesic reranking's generalization capability.",
      "## 4.2 Baselines\n\nWecompare Maniscope against four state-of-the-art reranking approaches representing different paradigms:\n\n- HNSW (Hierarchical Navigable Small World) : Graph-based approximate nearest neighbor search using hierarchical layers for efficient navigation. Selected as the primary graph-based baseline to evaluate whether geodesic distance on k-NN manifolds outperforms hierarchical navigation for reranking tasks.\n- Jina Reranker v2 : Transformer-based cross-encoder reranker [9] that jointly encodes querydocument pairs. Selected as a representative of specialized neural reranking models optimized for retrieval tasks with strong empirical performance on BEIR benchmarks.\n- BGE-M3 [27]: BAAI's general embedding model with multi-lingual, multi-functionality, and multi-granularity capabilities. Selected as a representative of advanced cross-encoder approaches that extend beyond basic cosine similarity.\n- LLM-Reranker (Gemini-2.0-Flash-Lite) : Represents the emerging class of LLM-based rerankers that leverage large language models for semantic scoring. Included as an upper-bound reference showing theoretical accuracy limits, though impractical for production due to 4-15s latency per query.\n\nThese baselines span the spectrum of reranking approaches-from graph-based (HNSW, Maniscope) to specialized cross-encoders (Jina v2, BGE-M3) to LLM-based (upper bound)-providing comprehensive comparison for evaluating different accuracy-efficiency tradeoffs.",
      "## 4.3 Evaluation Metrics\n\nWe evaluate using Precision@k (fraction of topk results that are relevant), Recall@k (fraction of all relevant documents in topk ), Mean Reciprocal Rank defined as MRR = 1 | Q | ∑ q ∈ Q 1 rank first relevant ( q ) , and Category Accuracy (percentage of queries where top-1 document has the correct category). Category Accuracy is particularly important for disambiguation, measuring whether the system retrieved the right type of content.",
      "## 4.4 Implementation Details\n\nWe use Sentence-BERT multilingual embeddings: paraphrase-multilingual-MiniLM-L12-v2 (384 dimensions) as the default model, with support for paraphrase-multilingual-mpnet-base-v2 (768d), multilingual-e5-large (1024d), and bge-m3 (1024d). The k-NN graph is constructed with k in (3, 5, 7, 9, 11, 13, 15) using cosine distance metric. Hybrid scoring parameter α is varied in (0.0, 0.25, 0.5, 0.75, 1.0). All experiments run on standard CPU with no GPU required for inference. The implementation uses Python 3.11, NetworkX for graph algorithms, scikit-learn for k-NN, hnswlib for HNSW baseline, and sentence-transformers for embeddings. All code, data, and experiments will be publicly available at https://github.com/ . Our Streamlit demo allows interactive exploration of geodesic reranking on custom queries and datasets.",
      "## 4.5 Experimental Protocol\n\nDocuments are embedded offline using the pre-trained model. For each query, we retrieve topM candidates using cosine similarity (Telescope phase), build a k-NN graph over these candidates, rerank using geodesic distances (Microscope phase), and compute evaluation metrics. We perform ablation studies by varying k and α to analyze parameter sensitivity.",
      "## 4.6 Research Questions\n\nOur evaluation addresses four key questions: (RQ1) Does geodesic reranking improve retrieval quality over cosine similarity? (RQ2) What is the optimal balance ( α ) between global cosine and local geodesic similarity? (RQ3) How does the choice of k (graph connectivity) affect performance? (RQ4) When does geodesic reranking help most, and what patterns emerge across different query types and document clusters?",
      "## 5 Results and Analysis\n\nWe evaluate Maniscope on eight BEIR-format benchmark datasets [23] spanning diverse domains: scientific claims (SciFact), web search (MS MARCO), COVID-19 research (TREC-COVID), counterarguments (ArguAna), financial QA (FiQA), semantic disambiguation (AorB), medical information retrieval (NFCorpus), and fact verification (FEVER). We compare against three practical baselines:\n\n- HNSW : Hierarchical Navigable Small World graph-based approximate nearest neighbor search (6-18ms latency)\n- Jina Reranker v2 : Transformer cross-encoder (6-67ms latency, varies by dataset)\n- BGE-M3 : BAAI general embedding M3 (33-291ms latency)\n\nWealso evaluate LLM-Reranker (Gemini-2.0-Flash-Lite) on TREC-COVID to establish a theoretical upper bound for achievable performance, though its latency (4.4s per query) makes it impractical for production use.",
      "## 5.1 Main Results: Eight-Dataset Evaluation\n\nTable 1 presents results across all eight datasets with optimized v2o implementation. Maniscope achieves competitive accuracy with graph-based (HNSW) and cross-encoder baselines while maintaining the lowest average latency (4.7ms).",
      "## 5.1.1 Key Findings\n\n- Maniscope wins on hardest datasets: NFCorpus (+7.0% NDCG@3 vs HNSW), TRECCOVID (+1.6%), AorB (+2.8%). These are domain-specific datasets where manifold structure captures semantic coherence that HNSW's hierarchical navigation misses.\n\nTable 1: Performance on 8 BEIR benchmarks (1,233 queries). Maniscope wins on 3 hardest datasets (NFCorpus, TREC-COVID, AorB) with 3.2 × speedup over HNSW. Bold = best, underline = second best.\n\n| Dataset                 | Queries   | ReRanker   |    MRR |   NDCG@3 |    P@3 |   Latency (ms) | vs HNSW Speedup   |\n|-------------------------|-----------|------------|--------|----------|--------|----------------|-------------------|\n| NFCorpus (Medical)      | 323       | Maniscope  | 0.8247 |   0.7063 | 0.4742 |            4.6 | 3.7 ×             |\n|                         |           | HNSW       | 0.8237 |   0.6602 | 0.4454 |           17   | 1.0 ×             |\n|                         |           | Jina v2    | 0.848  |   0.6718 | 0.4537 |           62.3 | 0.27 ×            |\n|                         |           | BGE-M3     | 0.7951 |   0.6025 | 0.4062 |          271.8 | 0.06 ×            |\n| TREC-COVID (Biomedical) | 50        | Maniscope  | 1      |   0.9659 | 0.9267 |            4.5 | 3.9 ×             |\n|                         |           | HNSW       | 1      |   0.9506 | 0.92   |           17.4 | 1.0 ×             |\n|                         |           | Jina v2    | 0.99   |   0.9635 | 0.9267 |           64.9 | 0.27 ×            |\n|                         |           | BGE-M3     | 1      |   0.9906 | 0.9467 |          284.9 | 0.06 ×            |\n| AorB (Disambiguation)   | 50        | Maniscope  | 0.9483 |   0.8698 | 0.7733 |            4.4 | 1.4 ×             |\n| AorB (Disambiguation)   | 50        | HNSW       | 0.9533 |   0.8463 | 0.7467 |            6   | 1.0 ×             |\n| AorB (Disambiguation)   | 50        | Jina v2    | 1      |   0.9316 | 0.8467 |            5.8 | 1.0 ×             |\n| AorB (Disambiguation)   | 50        | BGE-M3     | 0.9767 |   0.8953 | 0.8    |           31.9 | 0.19 ×            |\n| SciFact (Scientific)    | 100       | Maniscope  | 0.9708 |   0.9739 | 0.8833 |            4.6 | 3.8 ×             |\n| SciFact (Scientific)    | 100       | HNSW       | 0.9717 |   0.9789 | 0.88   |           17.3 | 1.0 ×             |\n| SciFact (Scientific)    | 100       | Jina v2    | 0.98   |   0.9852 | 0.89   |           62.2 | 0.28 ×            |\n| SciFact (Scientific)    | 100       | BGE-M3     | 0.9742 |   0.9763 | 0.8833 |          275   | 0.06 ×            |\n| ArguAna (Arguments)     | 100       | Maniscope  | 0.9912 |   0.99   | 0.9533 |            5.4 | 3.3 ×             |\n| ArguAna (Arguments)     | 100       | HNSW       | 0.995  |   0.9963 | 0.9633 |           17.7 | 1.0 ×             |\n| ArguAna (Arguments)     | 100       | Jina v2    | 0.99   |   0.9926 | 0.9567 |           65.1 | 0.27 ×            |\n| ArguAna (Arguments)     | 100       | BGE-M3     | 0.977  |   0.9789 | 0.94   |          283.4 | 0.06 ×            |\n| FiQA (Financial)        | 100       | Maniscope  | 0.9814 |   0.9795 | 0.92   |            4.5 | 3.7 ×             |\n| FiQA (Financial)        | 100       | HNSW       | 0.9803 |   0.9758 | 0.92   |           16.8 | 1.0 ×             |\n| FiQA (Financial)        | 100       | Jina v2    | 0.99   |   0.9862 | 0.93   |           57   | 0.29 ×            |\n| FiQA (Financial)        | 100       | BGE-M3     | 0.985  |   0.9816 | 0.9267 |          254.6 | 0.07 ×            |\n| MS MARCO (Web Search)   | 200       | Maniscope  | 1      |   1      | 1      |            4.6 | 2.4 ×             |\n| MS MARCO (Web Search)   | 200       | HNSW       | 1      |   1      | 1      |           11.2 | 1.0 ×             |\n| MS MARCO (Web Search)   | 200       | Jina v2    | 1      |   1      | 1      |           16.3 | 0.69 ×            |\n| MS MARCO (Web Search)   | 200       | BGE-M3     | 1      |   1      | 1      |           87.8 | 0.13 ×            |\n| FEVER (Fact Check)      | 200       | Maniscope  | 0.9975 |   0.9978 | 0.9917 |            4.7 | 3.1 ×             |\n| FEVER (Fact Check)      | 200       | HNSW       | 0.9975 |   0.9978 | 0.9917 |           14.6 | 1.0 ×             |\n| FEVER (Fact Check)      | 200       | Jina v2    | 1      |   1      | 1      |           42.5 | 0.34 ×            |\n| FEVER (Fact Check)      | 200       | BGE-M3     | 1      |   1      | 1      |          190.5 | 0.08 ×            |\n| Average                 | 1,233     | Maniscope  | 0.9642 |   0.9354 | 0.8653 |            4.7 | 3.2 ×             |\n| Average                 | 1,233     | HNSW       | 0.9652 |   0.9287 | 0.8587 |           14.8 | 1.0 ×             |\n| Average                 | 1,233     | Jina v2    | 0.9748 |   0.9539 | 0.893  |           47   | 0.31 ×            |\n| Average                 | 1,233     | BGE-M3     | 0.9632 |   0.9532 | 0.8778 |          210   | 0.07 ×            |\n\n- Speed champion: Maniscope achieves 4.4-5.4ms latency (avg 4.7ms), making it 3.2 × faster than HNSW on average, 10-45 × faster than cross-encoders, and 420 × faster than LLM-Reranker.\n- Near upper-bound accuracy: On TREC-COVID, LLM-Reranker (theoretical upper bound) achieves NDCG@3 0.9835, only +1.8% better than Maniscope (0.9659) but at 420 × latency penalty. This demonstrates Maniscope captures most of the achievable semantic quality at practical speed.\n- Competitive overall: On easier datasets (MS MARCO, FEVER), all methods achieve nearperfect scores. Maniscope ties with HNSW while being faster.\n- Graph-based paradigm validated: Both Maniscope and HNSW achieve competitive accuracy with cross-encoders at 6-29 × lower latency, challenging the assumption that expensive crossencoders are always necessary.",
      "## 5.2 Upper Bound Analysis: LLM-Reranker on TREC-COVID\n\nTable 2: Five-way comparison on TREC-COVID (biomedical domain, 50 queries). LLM-Reranker (Gemini-2.0-Flash-Lite) provides +1.8% NDCG@3 improvement over Maniscope at 420 × higher latency, demonstrating theoretical upper bound but impractical deployment cost. Bold = best.\n\n| ReRanker         |   MRR |   NDCG@3 | Latency (ms)   | vs Maniscope   |\n|------------------|-------|----------|----------------|----------------|\n| Maniscope        |  1    |   0.9659 | 10.5           | 1.0 ×          |\n| HNSW             |  1    |   0.9506 | 17.5           | 1.7 × slower   |\n| Jina Reranker v2 |  0.99 |   0.9635 | 64.9           | 6.2 × slower   |\n| BGE-M3           |  1    |   0.9906 | 285.6          | 27 × slower    |\n| LLM-Reranker     |  1    |   0.9835 | 4,410.6        | 420 × slower   |\n\nInterpretation: LLM-Reranker achieves the second-best NDCG@3 (0.9835), outperforming Maniscope by +1.8% absolute (0.9835 vs 0.9659) but requiring 420 × higher latency (4.4 seconds vs 10.5ms per query). This positions LLM-Reranker as a theoretical upper bound showing what's achievable with unlimited computational budget, but impractical for production RAG systems where sub-100ms latency is critical. BGE-M3 cross-encoder achieves the best NDCG@3 (0.9906, +2.6% over Maniscope) but still requires 27 × higher latency. Maniscope achieves competitive accuracy within 2-3% of theoretical upper bounds while maintaining real-time performance.",
      "## · Different graph philosophies:\n\n- -HNSW: Hierarchical layers optimized for approximate nearest neighbor search across millions of documents\n- -Maniscope: Flat k-NN graph capturing local manifold structure for refinement of top-k candidates",
      "## · Different distance semantics:\n\n- -HNSW: Greedy routing through hierarchical graph (optimized for speed, not semantic coherence)\n- -Maniscope: Geodesic paths (shortest paths on manifold preserve semantic relationships)",
      "## · When geodesic helps:\n\n- -Domain-specific terminology (NFCorpus medical: +7.0% NDCG@3)\n\n- -Semantic disambiguation (AorB: +2.8%)\n- -Complex clustering structure (TREC-COVID: +1.6%)",
      "## · When it's less critical:\n\n- -Simple factual retrieval (MS MARCO: tie)\n- -High-quality embeddings suffice (FEVER: tie)\n- -Clear relevance signals (ArguAna: -0.6%, SciFact: -0.5%)\n\nSpeed advantage: Maniscope's flat k-NN graph with vectorized scipy operations is 2 × faster than HNSW's hierarchical navigation on average (7.7ms vs 14.9ms).",
      "## 5.4 Cross-Encoder Comparison\n\nWhile cross-encoders (Jina v2, BGE-M3) achieve marginally better average accuracy (+1.1% NDCG@3), their benefits come at significant cost:\n\n- Jina v2: +1.9% NDCG@3, but 6 × slower than Maniscope\n- BGE-M3: +1.9% NDCG@3, but 29 × slower than Maniscope\n- Exception: Maniscope beats both cross-encoders on NFCorpus (+5.1% vs Jina, +17.2% vs BGE-M3)\n\nPractical implication: For production RAG systems requiring sub-10ms latency, graph-based methods (Maniscope, HNSW) provide the best speed/quality tradeoff.",
      "## 5.5 Statistical Analysis\n\nWe perform paired Wilcoxon signed-rank tests comparing Maniscope vs each baseline across all 1,233 queries:\n\n- vs HNSW (NDCG@3): Maniscope wins on 3 datasets, ties on 2, loses marginally on 2 (p=0.12, not statistically significant overall, but wins on hardest datasets)\n- vs HNSW (Latency): Maniscope is 1.9 × faster on average (p ¡ 0.001, highly significant)\n- vs Jina v2: Maniscope achieves -1.9% lower NDCG@3 (p=0.08, not significant), with 6 × average speedup (p ¡ 0.001)\n- vs BGE-M3: Maniscope achieves -1.9% lower NDCG@3 (p=0.09, not significant), with 29 × speedup (p ¡ 0.001)\n\nConclusion: Maniscope achieves statistically equivalent accuracy to all baselines while providing significant speedup advantages. On challenging domain-specific datasets, Maniscope's geodesic distance approach captures manifold structure that HNSW's navigation and cross-encoder attention miss.",
      "## 6.1 When Does Geodesic Reranking Help?\n\nOur results suggest geodesic reranking is most beneficial in three scenarios. First, when semantic clusters exist in the document collection, where documents naturally form distinct groups (e.g., programming vs. animals), geodesic distances respect cluster boundaries while cosine similarity may leak across clusters due to shared vocabulary. Second, when mid-range ranking matters, geodesic reranking shows its strength at positions 3-5 where local manifold structure refines ranking, even though top-1 performance is often similar between methods. This is particularly important for RAG systems that use multiple retrieved documents. Third, when queries are ambiguous and contain terms from multiple semantic clusters (like 'What does a python eat?'), geodesic reranking maintains coherence within the correct cluster, with anchor node selection guiding the local search toward the most relevant semantic neighborhood.",
      "## 6.2 Cluster Disconnection in k-NN Graphs\n\nWhen extending retrieval beyond top-5 in the snake/python disambiguation benchmark, geodesic scores drop to exactly 0.000 for out-of-cluster documents. For the query 'What does a python eat?' with top-10 retrieval, ranks 0-7 (all snake-animal documents) have geodesic scores ranging from 1.000 to 0.723, while ranks 8-9 (both programming documents) have geodesic scores of exactly 0.000.\n\nZero geodesic score indicates no path exists in the k-NN graph between the anchor node (snake 03) and the programming documents. The k-NN graph ( k = 5) forms disconnected components corresponding to semantic clusters, with geodesic distance acting as a cluster boundary detector.\n\nObserved properties: (1) Geodesic scores identify when documents belong to different semantic clusters in this dataset. (2) Zero geodesic scores flag potential cross-cluster contamination. (3) Hybrid scoring ( α = 0 . 5) retains cosine contribution even when geodesic=0.0, maintaining a soft fallback. (4) Smaller k creates stronger cluster boundaries with more disconnection; larger k improves connectivity but may blur boundaries.\n\nThese observations on our small disambiguation dataset suggest that k-NN graphs can capture semantic cluster structure. Disconnected components provide topological information about the embedding space rather than representing a limitation.",
      "## 6.3 Comparison to Related Approaches\n\nCompared to HNSW, which uses hierarchical graph layers for efficient approximate nearest neighbor search, our method constructs a flat k-NN manifold graph optimized for capturing local semantic structure. HNSW excels at large-scale retrieval (millions of documents) through greedy hierarchical navigation, while geodesic reranking excels at small-candidate refinement (10-100 documents) by computing exact shortest paths on manifold graphs. Our results show Maniscope outperforms HNSW on semantically challenging datasets (NFCorpus: +7.0%, TREC-COVID: +1.6%, AorB: +2.8% NDCG@3) while being 3.2 × faster, suggesting geodesic distance better captures manifold structure than hierarchical navigation for reranking tasks.\n\nCompared to cross-encoders, which provide stronger reranking but require O ( N ) forward passes per query, our method operates at O ( M ) where M ≪ N by reranking only top candidates. Geodesic reranking could complement cross-encoders in a coarse → geodesic → cross-encoder pipeline.\n\nCompared to ColBERT, which performs late interaction between query and document tokens, our method operates on sentence-level embeddings, offering a trade-off between expressiveness and efficiency with simpler and faster computation.\n\nCompared to graph-based retrieval methods like PageRank and random walks that focus on link structure in citation graphs, our k-NN graph is built purely from embeddings without external links, and geodesic distance is deterministic rather than probabilistic.",
      "## 7.1 Limitations\n\nOur work has several limitations that warrant acknowledgment and suggest directions for future investigation.\n\nEvaluation scale. Our experiments evaluate 600 queries across six BEIR-format datasets. While these benchmarks are standard in the retrieval literature and span diverse domains, the relatively small query count (50-200 per dataset) limits statistical power for detecting small performance differences. Large-scale evaluation on thousands of queries per domain would strengthen confidence in the reported improvements.\n\nCorpus size considerations. The initial retrieval stage requires O ( ND ) cosine similarity computation over the full corpus. For very large corpora ( N &gt; 10 7 documents), this can become a bottleneck. While this limitation is shared with all dense retrieval methods and can be mitigated using approximate nearest neighbor algorithms (FAISS, Annoy), we do not evaluate such integration. The reranking stage operates on M candidates where M ≪ N , making it scalable, but the end-to-end latency depends on efficient first-stage retrieval.\n\nHyperparameter sensitivity. The method introduces two hyperparameters: graph connectivity k and hybrid scoring weight α . Our experiments show α ≈ 0 . 5 and k ∈ { 3 , 5 , 7 } work well across datasets, but optimal values may vary by domain and query type. We do not provide a principled method for selecting these parameters beyond empirical grid search. Query-dependent or learned parameter selection could improve robustness.\n\nEmbedding quality dependency. Geodesic reranking operates on pre-computed embeddings and assumes they capture meaningful semantic structure. If the underlying embedding model produces poor representations (e.g., failing to separate distinct semantic clusters), geodesic distances cannot recover. The method does not address the fundamental embedding quality problem; it enhances retrieval given reasonable embeddings.\n\nDisconnected graph components. When k is small or semantic clusters are well-separated, the k-NN graph may fragment into disconnected components. Documents unreachable from the anchor node receive geodesic score 0.0, causing the final score to revert to cosine similarity. While we frame this as a cluster boundary detection feature, it also means geodesic information is lost for cross-cluster documents. Increasing k improves connectivity but may blur semantic boundaries.\n\nLinguistic and cultural coverage. Our evaluation uses primarily English datasets. The method's performance on non-English languages or multilingual retrieval tasks is unknown. Additionally, semantic ambiguity patterns (like 'Python' or 'Apple') may differ across languages and cultures, potentially affecting generalization of the disambiguation results.\n\nQuery characteristics. We do not systematically analyze performance variation across query types (keyword vs. natural language), query length (1-2 words vs. full sentences), or query specificity (broad exploratory vs. narrow factual). The AorB disambiguation benchmark uses carefully constructed ambiguous queries, but real-world query distributions may differ.",
      "## 7.2 Future Work\n\nThe limitations above suggest several potential research directions:\n\n- Multi-anchor ensemble methods: Using multiple anchor nodes rather than a single topranked document could improve robustness when queries have multiple valid interpretations or when the top result is ambiguous.\n- Adaptive hyperparameter selection: Learning query-dependent weights for hybrid scoring could improve performance across diverse query types and domains, addressing the current reliance on fixed hyperparameters.\n- Alternative manifold metrics: Exploring diffusion distances [16], commute times, or resistance distances on k-NN graphs could provide different perspectives on manifold geometry and cluster connectivity.\n- Multilingual and cross-lingual retrieval: Evaluating the method on non-English and multilingual datasets would validate generalization across languages and cultural contexts.",
      "## 8 Conclusion\n\nWe introduced Geodesic Reranking, a method that combines global cosine similarity with local geodesic distances on k-NN manifolds for document reranking in RAG systems.",
      "## 8.1 Key Contributions\n\n- Method: Application of geodesic distances on k-NN manifolds for reranking, capturing local manifold structure\n- Empirical results: On eight BEIR-format datasets (1,233 queries), average MRR 0.9691, NDCG@3 0.9199. Maniscope outperforms HNSW graph-based baseline on three hardest datasets (NFCorpus: +7.0%, TREC-COVID: +1.6%, AorB: +2.8% NDCG@3) while being 2 × faster\n\n(6.9ms vs 13.4ms average). Achieves within 2% accuracy of cross-encoders at 6-29 × lower latency.\n\n- Implementation: O ( ND + M 2 D + Mk log k ) per query where M ≪ N , with open-source release\n- Analysis: Demonstration that disconnected components in k-NN graphs correspond to semantic clusters in small-scale disambiguation tasks",
      "## 8.2 Implications\n\nThis work demonstrates how geometric insights can drive algorithmic efficiency, which cascades into energy savings and ultimately contributes to more sustainable AI systems. We discuss these interconnected themes below.",
      "## 8.2.1 Geometric Insight to Algorithmic Efficiency\n\nGeodesic reranking addresses a limitation of cosine similarity-based dense retrieval: treating embeddings as flat Euclidean spaces. Computing geodesic distances on k-NN manifolds captures local manifold structure.\n\nOn our disambiguation benchmark, this approach: (1) Disambiguates semantically similar terms (python-as-snake vs. Python-as-language), (2) Maintains ranking coherence within semantic clusters, (3) Improves mid-range retrieval quality (P@3-5).\n\nOur results are consistent with the manifold hypothesis [22]: embeddings lie on lower-dimensional manifolds. Cosine similarity measures global angular distance, while geodesic distance is path-based and respects local neighborhoods. Hybrid scoring parameter α balances these views.\n\nThe computational cost is manageable: geodesic computation adds O ( Mk log k ) reranking complexity, negligible compared to initial retrieval O ( ND ). Cross-encoder rerankers require O ( N ) forward passes through neural networks, each 100-1000 × more expensive than distance computation.",
      "## 8.2.2 Energy Efficiency\n\nThe measured speedups over practical reranking baselines imply corresponding reductions in energy consumption. We estimate this impact using measured latencies from eight BEIR benchmarks (1,233 queries across all datasets). Wefocus on production-viable methods (Maniscope, HNSW, cross-encoders) and exclude LLM-Reranker, which is impractical for deployment due to 4.4-second latency per query (420 × slower than Maniscope).\n\nEnergy model: We estimate environmental impact for a RAG system processing Q = 10 6 queries/day with top-10 candidate reranking. Assuming NVIDIA T4 GPU (70W active inference power) and US grid carbon intensity (0.39 kg CO 2 /kWh). These estimates represent lower bounds; actual consumption depends on hardware configuration, batch size, and system utilization.",
      "## Daily energy consumption (1M queries/day):\n\n<!-- formula-not-decoded -->",
      "## Annual carbon footprint (1M queries/day):\n\n```\nManiscope = 0 . 019 tons CO 2 /year HNSW = 0 . 037 tons CO 2 /year (+95% vs Maniscope) Jina v2 = 0 . 12 tons CO 2 /year (+532% vs Maniscope) BGE-M3 = 0 . 56 tons CO 2 /year (+2 , 847% vs Maniscope)\n```\n\nImpact at scale: Extrapolating to 100M queries/day (a realistic volume for enterprise RAG systems), Maniscope would save 1.8 tons CO 2 /year vs. HNSW, 10.3 tons CO 2 /year vs. Jina v2,\n\nand 54.1 tons CO 2 /year vs. BGE-M3. The 2 × speedup over HNSW translates to halving energy consumption, while the 29 × speedup over BGE-M3 reduces carbon emissions by 97%.",
      "## 8.2.3 Environmental Sustainability\n\nThis work applies geometric principles (manifold structure of embeddings) to achieve algorithmic efficiency rather than scaling model size or compute. The measured speedups translate to energy reductions at deployment scale.\n\nModern AI systems often emphasize computational scaling-larger models, more GPUs, longer training. This approach incurs environmental costs. Geodesic reranking represents an alternative: exploiting geometric structure in the problem yields orders-of-magnitude gains in speed and energy efficiency relative to neural baselines, while maintaining competitive accuracy.\n\nThe results suggest that algorithmic approaches informed by geometric understanding can complement or provide alternatives to parameter scaling in retrieval tasks. For deployment scenarios where latency and energy matter, methods that exploit problem structure merit consideration alongside neural approaches.",
      "## Acknowledgments\n\nThe author thanks the open-source community for foundational tools including sentence-transformers, NetworkX, scikit-learn, FlagEmbedding, and PHATE. Anthropic's Claude was used for literature search, manuscript editing, coding and debugging tasks. Google's Gemini helped prepare 'AorB' dataset. All scientific analyses, interpretations, and conclusions are the author's own.",
      "## References\n\n- [1] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K¨ uttler, Mike Lewis, Wen-tau Yih, Tim Rocktaschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems , 33:9459-9474, 2020.\n- [2] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pages 4171-4186, 2019.\n- [3] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bertnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing , 2019.\n- [4] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 6769-6781, 2020.\n- [5] Sanjeev Arora, Mikhail Khodak, Nikunj Saunshi, and Kiran Vodrahalli. A compressed sensing view of unsupervised text embeddings, bag-of-n-grams, and lstms. International Conference on Learning Representations , 2018.\n- [6] Kawin Ethayarajh. How contextual are contextualized word representations? comparing the geometry of bert, elmo, and gpt-2 embeddings. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , pages 55-65, 2019.\n\n- [7] Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett, Junaid Ahmed, and Arnold Overwijk. Approximate nearest neighbor negative contrastive learning for dense text retrieval. International Conference on Learning Representations (ICLR) , 2021.\n- [8] Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Few-shot learning with retrieval augmented language models. arXiv preprint arXiv:2208.03299 , 2022.\n- [9] Rodrigo Nogueira and Kyunghyun Cho. Passage re-ranking with bert. In arXiv preprint arXiv:1901.04085 , 2019.\n- [10] Omar Khattab and Matei Zaharia. Colbert: Efficient and effective passage search via contextualized late interaction over bert. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval , pages 39-48, 2020.\n- [11] Chen Qin, Ruqing Zhang, Jiafeng Kuang, Xueqi Xie, Yixing Guo, and Xueqi Cheng. Are neural ranking models robust? ACM Transactions on Information Systems , 2021.\n- [12] Joshua B Tenenbaum, Vin De Silva, and John C Langford. A global geometric framework for nonlinear dimensionality reduction. science , 290(5500):2319-2323, 2000.\n- [13] Sam T Roweis and Lawrence K Saul. Nonlinear dimensionality reduction by locally linear embedding. science , 290(5500):2323-2326, 2000.\n- [14] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research , 9(11), 2008.\n- [15] Leland McInnes, John Healy, and James Melville. Umap: Uniform manifold approximation and projection for dimension reduction. arXiv preprint arXiv:1802.03426 , 2018.\n- [16] Kevin R Moon, David van Dijk, Zheng Wang, Scott Gigante, Daniel B Burkhardt, William S Chen, Kristina Yim, Antonia van den Elzen, Matthew J Hirn, Ronald R Coifman, et al. Visualizing structure and transitions in high-dimensional biological data. Nature biotechnology , 37(12):14821492, 2019.\n- [17] Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. The pagerank citation ranking: Bringing order to the web. 1999.\n- [18] Hanghang Tong, Christos Faloutsos, and Jia-Yu Pan. Fast random walk with restart and its applications. In Sixth International Conference on Data Mining (ICDM'06) , pages 613-622. IEEE, 2006.\n- [19] Xiang Wang, Xiangnan He, Yixin Cao, Meng Liu, and Tat-Seng Chua. Kgat: Knowledge graph attention network for recommendation. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining , pages 950-958, 2019.\n- [20] Zhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian Tang. Rotate: Knowledge graph embedding by relational rotation in complex space. In International Conference on Learning Representations , 2019.\n- [21] Yu A Malkov and Dmitry A Yashunin. Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs. In IEEE transactions on pattern analysis and machine intelligence , volume 42, pages 824-836, 2018.\n- [22] Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives. IEEE transactions on pattern analysis and machine intelligence , 35(8):1798-1828, 2013.\n- [23] Nandan Thakur, Nils Reimers, Andreas R¨ uckl´ e, Abhishek Srivastava, and Iryna Gurevych. Beir: A heterogeneous benchmark for zero-shot evaluation of information retrieval models. Advances in Neural Information Processing Systems , 34:9354-9366, 2021.\n\n- [24] David Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu Wang, Madeleine van Zuylen, Arman Cohan, and Hannaneh Hajishirzi. Fact or fiction: Verifying scientific claims. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 986-1000, 2020.\n- [25] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. Ms marco: A human generated machine reading comprehension dataset. arXiv preprint arXiv:1611.09268 , 2016.\n- [26] Ellen Voorhees, Tasmeer Alam, Steven Bedrick, Dina Demner-Fushman, William R Hersh, Kyle Lo, Kirk Roberts, Ian Soboroff, and Lucy Lu Wang. Trec-covid: constructing a pandemic information retrieval test collection. In ACM SIGIR Forum , volume 54, pages 1-12, 2021.\n- [27] Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. Bge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation. arXiv preprint arXiv:2402.03216 , 2024.",
      "## A.1 AorB Dataset\n\nMotivation: To systematically evaluate Maniscope's ability to handle semantic disambiguation at scale, we developed a BEIR-compatible dataset ('A or B') where ambiguous keywords require contextual understanding for correct retrieval.\n\nDataset Design: AorB contains 200 documents and 50 queries spanning six ambiguous term pairs:\n\n- Python : Snake biology vs. Programming language\n- Apple : Fruit/agriculture vs. Technology company\n- Java : Indonesian island/coffee vs. Programming language\n- Mercury : Planet (astronomy) vs. Chemical element (chemistry)\n- Jaguar : Big cat (zoology) vs. Luxury automobile\n- Flow : Fluid dynamics (physics) vs. Business workflow\n\nEach query includes balanced hard negatives: documents from both semantic clusters that share the ambiguous keyword. Standard cosine similarity struggles because lexical overlap does not guarantee semantic relevance.\n\nResults: On AorB (50 queries), Maniscope achieves MRR 0.9483 with 10.4ms latency, demonstrating 453 × speedup over LLM-Reranker (MRR 1.0000, 4,710ms) while maintaining competitive quality. Jina v2 achieves MRR 1.0000 at 6.8ms, showing that careful hyperparameter tuning of embedding-based methods can match geodesic reranking on disambiguation tasks. This validates AorB as a challenging benchmark that differentiates retrieval methods.",
      "## A.2 Parameter Sensitivity Analysis\n\nWe conduct grid search experiments to understand the impact of key hyperparameters k (graph connectivity) and α (hybrid scoring weight) on retrieval quality across multiple metrics.\n\nWe performed a systematic grid search on the AorB dataset (50 queries) with parameter ranges: k ∈ { 3 , 5 , 7 , 9 , 11 } and α ∈ { 0 . 0 , 0 . 25 , 0 . 5 , 0 . 75 , 1 . 0 } (25 experiments total).\n\nTable 3: Grid Search Results: Impact of α on AorB Dataset ( k has no effect)\n\n| α                    |    MRR |   NDCG@3 |    P@3 |    MAP |\n|----------------------|--------|----------|--------|--------|\n| 0.00 (Pure Geodesic) | 0.9417 |   0.8829 | 0.8667 | 0.9306 |\n| 0.25 (Optimal)       | 0.9483 |   0.8876 | 0.8733 | 0.9263 |\n| 0.50 (Hybrid)        | 0.9483 |   0.8698 | 0.8533 | 0.9136 |\n| 0.75                 | 0.9483 |   0.8498 | 0.8267 | 0.9052 |\n| 1.00 (Pure Cosine)   | 0.9533 |   0.8463 | 0.82   | 0.9022 |",
      "## Key Observations:\n\n- Complete k insensitivity: Graph connectivity k has zero impact on AorB - for all k ∈ { 3 , 5 , 7 , 9 , 11 } , metrics are identical (to 4 decimal places) for any fixed α . This confirms that AorB's semantic clusters are well-separated, making local manifold structure unnecessary. Any k ≥ 3 produces the same results.\n- α = 0 . 25 is optimal for most metrics: The hybrid weight α = 0 . 25 (25% cosine, 75% geodesic) achieves the best balance:\n\n- -Best NDCG@3: 0.8876 (0.5% improvement over pure geodesic)\n- -Best P@3: 0.8733 (0.8% improvement over pure geodesic)\n- -Strong MRR: 0.9483 (only 0.5% below pure cosine)\n- -Strong MAP: 0.9263 (only 0.5% below pure geodesic)",
      "## · Metric-dependent tradeoffs:\n\n- -For MRR maximization: Use α = 1 . 0 (pure cosine) → 0.9533\n- -For NDCG@3/P@3 maximization: Use α = 0 . 25 → 0.8876/0.8733\n- -For MAP maximization: Use α = 0 . 0 (pure geodesic) → 0.9306\n- -For balanced performance: Use α = 0 . 25 (best overall compromise)\n- Smooth degradation: Performance degrades gracefully as α increases from 0.25 to 1.0, with NDCG@3 dropping from 0.8876 to 0.8463 (4.7% decrease) while MRR improves slightly from 0.9483 to 0.9533 (0.5% increase).\n- Recommended default: For AorB-like disambiguation tasks, use α = 0 . 25 with any k ≥ 3. The geodesic-heavy weighting captures semantic structure better than pure cosine similarity for multi-metric optimization.",
      "## B.1 Technical Details\n\nThe iterations from v0 (baseline) through v1, v2, v3 (cached) to v2o (production-optimized) demonstrate how algorithmic insights can achieve orders-of-magnitude performance improvements while preserving accuracy. We present the optimization trajectory v0 → v1 → v2 → v3 → v2o and key techniques that enabled triple-digit speedup.",
      "## B.2.1 Baseline Implementation (v0)\n\n```\ndef compute_geodesic_v0(query, candidates, k=5): # Build k-NN graph using NetworkX G = nx.Graph() for i, doc_i in itemize(candidates): for j, doc_j in itemize(candidates): if i != j: sim = cosine_similarity(doc_i, doc_j) G.add_edge(i, j, weight=1 -sim) # Find k-NN neighbors (slow: O(M^2) edges) for node in G.nodes(): neighbors = sorted(G[node].items(), key=lambda x: x[1]['weight'])[:k] # Keep only k-NN edges # Dijkstra's algorithm via NetworkX anchor = find_anchor(query, candidates) distances = nx.single_source_dijkstra_path_length(G, anchor) return convert_to_scores(distances)\n```\n\nPerformance: 101ms average latency (baseline) Bottlenecks:\n\n- Dense graph construction: O ( M 2 D ) pairwise distances\n- NetworkX overhead for small graphs\n- Redundant edge filtering",
      "## B.2.2 Efficient k-NN Construction (v1)\n\n```\ndef compute_geodesic_v1(query, candidates, k=5): # Pre-compute all pairwise similarities (vectorized) sims = cosine_similarity_matrix(candidates) # O(M^2 D) # Build sparse k-NN graph directly G = nx.Graph() for i in range(len(candidates)): # Get k-nearest neighbors efficiently neighbors = np.argsort(-sims[i])[:k+1] # +1 for self for j in neighbors: if i != j: G.add_edge(i, j, weight=1 -sims[i,j]) # Dijkstra via NetworkX anchor = find_anchor(query, candidates) distances = nx.single_source_dijkstra_path_length(G, anchor) return convert_to_scores(distances)\n```\n\nPerformance: 5.7ms average latency (17.8 × speedup)\n\nKey Improvement: Vectorized similarity computation + direct k-NN construction eliminates redundant edge filtering.",
      "## B.2.3 Replace NetworkX with Heap-based Dijkstra (v2)\n\n```\ndef compute_geodesic_v2(query, candidates, k=5): # Same k-NN construction as v1 sims = cosine_similarity_matrix(candidates) neighbors = [np.argsort(-sims[i])[:k+1] for i in range(len(candidates))] # Custom heap-based Dijkstra (pure Python) import heapq anchor = find_anchor(query, candidates) distances = {anchor: 0.0} heap = [(0.0, anchor)] while heap: dist, u = heapq.heappop(heap) if dist > distances[u]: continue for v in neighbors[u]: if v == u: continue alt = dist + (1 -sims[u, v]) if v not in distances or alt < distances[v]: distances[v] = alt\n```\n\n```\nheapq.heappush(heap, (alt, v))\n```\n\nreturn convert\\_to\\_scores(distances)\n\nPerformance: 4.9ms average latency (22 × speedup)\n\nKey Improvement: Eliminates NetworkX graph object overhead for small candidate sets (M=10100).",
      "## B.2.4 Persistent Graph Caching (v3)\n\n```\nfrom functools import lru_cache class CachedManiscope: def __init__(self): self.graph_cache = {} @lru_cache(maxsize=1000) def get_graph(self, candidates_hash): # Reuse graph if same candidates appear multiple times if candidates_hash in self.graph_cache: return self.graph_cache[candidates_hash] # Build graph (same as v2o) graph = build_sparse_graph(candidates) self.graph_cache[candidates_hash] = graph return graph\n```\n\nPerformance: 0.5ms warm-cache latency (230 × speedup)\n\nTrade-off: Excellent for repeated experiments with same candidate sets, but 16 × slower on cold start due to hashing overhead. v2o is preferred for production due to consistent performance.",
      "## B.2.5 SciPy Sparse Matrix + Vectorized Scoring (v2o)\n\n```\nfrom scipy.sparse import csr_matrix from scipy.sparse.csgraph import dijkstra def compute_geodesic_v2o(query, candidates, k=5): # Build sparse CSR adjacency matrix directly M = len(candidates) sims = cosine_similarity_matrix(candidates) # O(M^2 D) # Construct CSR matrix for Dijkstra row, col, data = [], [], [] for i in range(M): neighbors = np.argsort(-sims[i])[:k+1] for j in neighbors: if i != j: row.append(i) col.append(j) data.append(1 -sims[i, j]) graph = csr_matrix((data, (row, col)), shape=(M, M)) # SciPy's optimized Dijkstra (C implementation) anchor = find_anchor(query, candidates) distances = dijkstra(graph, indices=anchor, directed=False) # Vectorized scoring (NumPy) scores = 1 / (1 + distances)\n```",
      "## return scores\n\nPerformance: 11ms average latency on 6-dataset benchmark (628 × speedup vs LLM) Key Improvements:\n\n- SciPy's C-optimized Dijkstra: 2-3 × faster than pure Python heap\n- Sparse CSR matrix: minimal memory overhead\n- Vectorized NumPy scoring: eliminates Python loops",
      "## B.3 Performance Comparison\n\nTable 4: Optimization Impact Across Versions (Average Latency)\n\n| Version                                                    | Latency (ms)   | Speedup vs v0   | Use Case             |\n|------------------------------------------------------------|----------------|-----------------|----------------------|\n| v0 (Baseline) v1 (Efficient v2 (Heap Dijkstra) v2o (SciPy) | 101.5          | 1.0 ×           | Reference            |\n| k-NN)                                                      | 5.7            | 17.8 ×          | Early optimization   |\n|                                                            | 4.9            | 22.0 ×          | Reduced overhead     |\n|                                                            | 11.0           | 9.2 ×           | Production           |\n| v3 (Cached, warm)                                          | 0.5            | 230 ×           | Repeated experiments |\n| LLM-Reranker                                               | 6,973          | -               | Baseline comparison  |\n| v2o Speedup                                                | -              | 628 ×           | vs LLM               |\n\nNote: v2o is slightly slower than v2 (11ms vs 4.9ms) due to SciPy initialization overhead on small graphs, but remains 628 × faster than LLM-Reranker while providing better numerical stability and production reliability.",
      "## B.4 Key Lessons from Optimization Journey\n\n- Profile before optimizing: v0 → v1 targeted the actual bottleneck (dense graph construction), achieving 18 × speedup. The complete journey v0 → v1 → v2 → v3 → v2o demonstrates iterative refinement.\n- Library choice matters at scale: NetworkX is flexible but adds overhead for small graphs ( M &lt; 100). SciPy's sparse matrices excel here.\n- Vectorization wins: NumPy/SciPy operations leverage BLAS/LAPACK for near-C performance in Python.\n- Cache when appropriate: v3 caching provides 230 × speedup for repeated queries but adds cold-start penalty. Production systems should use v2o for consistent latency.\n- Architecture determines cacheability: As demonstrated in Section B.5, Maniscope's datasetlevel architecture enables 126 × speedup through persistent caching, while query-level rerankers (HNSW, neural models) see minimal benefit (0-2%) due to incompatibility with cross-query reuse. This represents a fundamental advantage of manifold-based methods for production RAG systems.\n- Persistent storage enables production deployment: Disk-backed caching achieves 0% crossrun variance (Section B.5.2), making sub-2ms latency reproducible and reliable for real-world deployment.\n- Preserve accuracy: All versions achieve identical MRR=1.0000 on validation sets, demonstrating that algorithmic optimization need not compromise quality. All optimization versions are available in the public repository with benchmark scripts for reproducibility.",
      "## B.5 Persistent Caching: Dataset-Level vs Query-Level Architecture\n\nA critical distinction emerges between Maniscope and traditional rerankers when analyzing performance across repeated benchmark runs. We conducted identical benchmark runs on TREC-COVID (50 queries) and NFCorpus (323 queries) to evaluate the effectiveness of v2o optimizations across different reranker architectures.",
      "## B.5.1 Experimental Setup\n\nWe compared five reranker configurations on two BEIR datasets:\n\n- Maniscope v0: Baseline (CPU, rebuilds graph per query)\n- Maniscope v2o: Ultimate optimization (GPU + FAISS + persistent cache)\n- Maniscope: Configurable version (uses sidebar setting, typically v1/v2)\n- HNSW: Graph-based baseline (CPU, no caching)\n- HNSW v2o: Optimized with GPU embeddings + caching\n\nEach configuration was benchmarked twice on the same datasets without restarting the application, simulating a production environment where the same document collections are queried multiple times.",
      "## B.5.2 Benchmark Results: Persistent Cache Validation\n\nTable 5: Cross-Run Latency Consistency (ms per query, mean across dataset)\n\n| Reranker           | TREC-COVID (50q)   | TREC-COVID (50q)   | NFCorpus (323q)   | NFCorpus (323q)   |\n|--------------------|--------------------|--------------------|-------------------|-------------------|\n|                    | Run 1              | Run 2              | Run 1             | Run 2             |\n| Maniscope v0       | 190.7              | 227.7              | 161.5             | 176.6             |\n| Maniscope          | 13.1               | 21.0               | 12.0              | 12.6              |\n| Maniscope v2o      | 1.8                | 1.8                | 1.8               | 1.8               |\n| HNSW               | 156.0              | 195.0              | 138.3             | 147.0             |\n| HNSW v2o           | 157.5              | 188.9              | 138.4             | 149.2             |\n| Variance Analysis: | Variance Analysis: | Variance Analysis: |                   |                   |\n| Maniscope v2o 0.0% | Maniscope v2o 0.0% | variance           | 0.0% variance     | 0.0% variance     |\n| HNSW +25%          | HNSW +25%          | variance           | +6.3% variance    | +6.3% variance    |\n| HNSW v2o           | HNSW v2o           | +20% variance      | +7.8% variance    | +7.8% variance    |\n\nKey Observation: Maniscope v2o exhibits perfect latency consistency (1.8ms in all runs), while all other rerankers show 6-25% variance across runs. This validates the effectiveness of persistent disk caching for manifold structures.",
      "## B.5.3 Analysis: Why Maniscope v2o is Unique\n\n126 × Speedup with Dataset-Level Caching Comparing baseline to v2o on Run 2 (TRECCOVID):\n\n<!-- formula-not-decoded -->\n\nThis speedup stems from Maniscope's dataset-oriented architecture :\n\n1. First Query: Build k-NN graph for entire document collection, cache to disk\n2. Subsequent Queries: Load cached graph from disk (1.8ms), reuse for all 50 queries\n\n3. After Restart: Persistent cache survives, still 1.8ms (0% variance)\n\nIn contrast, traditional rerankers operate on query-specific document subsets :\n\n1. Each query retrieves different top-k documents from corpus\n2. Reranker must rebuild structures for each unique document set\n3. Cannot reuse across queries (different documents each time)\n4. No persistent cache benefit (every run is effectively cold start)\n\nHNSW: Why v2o Optimization Provides Minimal Benefit HNSW v2o adds GPU-accelerated embeddings and document caching, yet shows:\n\n- TREC-COVID: 156.0ms → 157.5ms (Run 1, 1% slower )\n- NFCorpus: 138.3ms → 138.4ms (Run 1, 0% change )\n- Cross-run variance: 20% (Run 1 → 2), no cache benefit\n\nRoot Cause: HNSW must rebuild its hierarchical graph index for each query's unique document set. While embeddings can be cached, the expensive graph construction ( ∼ 120ms) cannot be reused. The cache overhead (hash computation ∼ 1-2ms) partially cancels the minor embedding cache benefit.",
      "## B.5.4 Architectural Comparison\n\nTable 6: Dataset-Level vs Query-Level Caching Architectures\n\n| Property                                                                   | Maniscope v2o                                   | HNSW / Neural                                                      |\n|----------------------------------------------------------------------------|-------------------------------------------------|--------------------------------------------------------------------|\n| Caching Scope Cache Granularity Reusable Across Queries Persistent Storage | Dataset-level Entire k-NN graph Yes (same docs) | Query-level Embeddings only No (different docs) Memory (ephemeral) |\n| Cold Start (Run 1) Warm Cache (Run Speedup (Cold → Warm)                   | 20-30ms 1.8ms (0% var) 10-15 ×                  | 140-190ms (15-25% var) 0-2%                                        |\n|                                                                            | Disk (survives restart)                         |                                                                    |\n| 2+)                                                                        |                                                 | 150-200ms                                                          |\n| Production Benefit                                                         |                                                 |                                                                    |\n|                                                                            | Dramatic                                        | Minimal                                                            |",
      "## B.5.5 Multi-Stage Caching in Maniscope v2o\n\nThe 126 × speedup results from caching all expensive stages:\n\nTable 7: Per-Stage Latency Breakdown and Caching Impact\n\n| Stage                                                           | v0 Time             | v2o Time                | Optimization                                                                                         |\n|-----------------------------------------------------------------|---------------------|-------------------------|------------------------------------------------------------------------------------------------------|\n| Embeddings k-NN Graph (FAISS) Geodesic Distances Hybrid Scoring | 30ms 80ms 50ms 40ms | 0.1ms 0.2ms 0.1ms 0.4ms | GPU + sentence-transformers cache Persistent disk cache Cached with graph structure Vectorized NumPy |\n| Total Measured                                                  | 200ms 227.7ms       | 0.8ms 1.8ms             | 250 × theoretical max 126 × actual                                                                   |\n\nNote: Measured latency includes I/O overhead (1.0ms) not captured in algorithmic profiling, explaining the gap between theoretical (250 × ) and measured (126 × ) speedup.",
      "## C Original BEIR Results (Six Datasets)\n\nThis appendix preserves the original results from the initial evaluation on six BEIR datasets, conducted before expanding to eight datasets and adding HNSW baseline. These results are provided for historical reference and comparison.\n\nTable 8: Original performance on 6 BEIR benchmarks (600 queries). Maniscope achieves competitive accuracy with 628 × average speedup over LLM-Reranker. Bold = best, underline = best-tied.\n\n| Dataset    | Queries   | ReRanker     |    MRR |   NDCG@3 | Latency (ms)   | Speedup   |\n|------------|-----------|--------------|--------|----------|----------------|-----------|\n| SciFact    | 100       | Maniscope    | 0.9717 |   0.9752 | 10.0           | 1.0 ×     |\n| SciFact    |           | LLM-Reranker | 0.9758 |   0.9776 | 4,412          | 441 ×     |\n| SciFact    |           | Jina v2      | 0.98   |   0.9852 | 66.6           | 6.7 ×     |\n| SciFact    |           | BGE-M3       | 0.9742 |   0.9763 | 291.0          | 29 ×      |\n| MS MARCO   |           | Maniscope    | 1      |   1      | 10.1           | 1.0 ×     |\n| MS MARCO   |           | LLM-Reranker | 0.9956 |   0.995  | 4,592          | 454 ×     |\n| MS MARCO   | 200       | Jina v2      | 1      |   1      | 17.3           | 1.7 ×     |\n| MS MARCO   |           | BGE-M3       | 1      |   1      | 91.3           | 9.0 ×     |\n|            |           | Maniscope    | 1      |   0.9659 | 11.2           | 1.0 ×     |\n|            |           | LLM-Reranker | 1      |   0.9835 | 5,051          | 451 ×     |\n| TREC-COVID | 50        | Jina v2      | 0.99   |   0.9635 | 65.7           | 5.9 ×     |\n|            |           | BGE-M3       | 1      |   0.9906 | 287.9          | 26 ×      |\n|            |           | Maniscope    | 0.9912 |   0.99   | 11.3           | 1.0 ×     |\n|            |           | LLM-Reranker | 0.985  |   0.9889 | 8,023          | 710 ×     |\n| ArguAna    | 100       | Jina v2      | 0.99   |   0.9926 | 67.4           | 6.0 ×     |\n|            |           | BGE-M3       | 0.977  |   0.9789 | 291.8          | 26 ×      |\n|            |           | Maniscope    | 0.9811 |   0.9795 | 13.8           | 1.0 ×     |\n|            |           | LLM-Reranker | 0.9883 |   0.9826 | 15,052         | 1,091 ×   |\n| FiQA       | 100       | Jina v2      | 0.99   |   0.9862 | 60.2           | 4.4 ×     |\n|            |           | BGE-M3       | 0.985  |   0.9816 | 266.1          | 19 ×      |\n|            |           | Maniscope    | 0.9483 |   0.9417 | 10.4           | 1.0 ×     |\n|            |           | LLM-Reranker | 1      |   1      | 4,710          | 453 ×     |\n| AorB       | 50        | Jina v2      | 1      |   1      | 6.8            | 0.65 ×    |\n|            |           | BGE-M3       | 0.9767 |   0.9717 | 34.3           | 3.3 ×     |\n|            |           | Maniscope    | 0.9804 |   0.9754 | 11.1           | 1.0 ×     |\n|            |           | LLM-Reranker | 0.9908 |   0.9899 | 6,973          | 628 ×     |\n| Average    | 600       | Jina v2      | 0.9917 |   0.9928 | 47.3           | 4.3 ×     |\n|            |           | BGE-M3       | 0.9855 |   0.9832 | 210.4          | 19 ×      |",
      "## C.1 Key Observations from Original Evaluation\n\nThe original six-dataset evaluation established Maniscope's core value proposition:\n\n- 628 × average speedup vs LLM-Reranker: Geometric mean across all datasets ranged from 441 × (SciFact) to 1,091 × (FiQA)\n- Competitive accuracy: Average MRR 0.9804, within 1.1% of best baseline (Jina v2: 0.9917)\n- Consistent sub-15ms latency: 10-14ms across all datasets with v2o optimization\n- Best on ArguAna: Achieved highest MRR (0.9912) among all methods, including LLMReranker",
      "## C.2 Comparison with Eight-Dataset Results\n\nThe expanded evaluation with eight datasets (1,233 queries total) and HNSW baseline strengthened the paper's contributions:\n\n- Added HNSW comparison: Demonstrated clear differentiation on challenging datasets (NFCorpus: +7.0%, TREC-COVID: +1.6%, AorB: +2.8% NDCG@3)\n- Added NFCorpus &amp; FEVER: Validated performance on medical domain (323 queries) and fact verification (200 queries)\n- Repositioned LLM-Reranker: Changed from main baseline to upper-bound reference, acknowledging its impracticality (4-15s latency)\n- Refined narrative: Shifted from 'speed vs LLM' to 'graph-based paradigm' (Maniscope + HNSW) competing with cross-encoders\n\nThe eight-dataset evaluation provides stronger evidence for ICML submission by:\n\n1. Establishing clear technical differentiation from HNSW (geodesic vs hierarchical navigation)\n2. Demonstrating superiority on domain-specific datasets where manifold structure matters\n3. Validating the graph-based reranking paradigm with two complementary approaches (HNSW for search, Maniscope for refinement)",
      "## Experimental Setup\n\nTo verify reproducibility and measure performance stability, we conducted controlled experiments with three benchmark runs on the same datasets:\n\n- Cold run : First execution after system restart, models loaded from disk\n- Warm run-1 : Second execution with models cached in GPU memory\n- Warm run-2 : Third execution to verify consistency",
      "## Configuration:\n\n- Hardware: NVIDIA GeForce GTX 1080 Ti (11GB)\n- Software: PyTorch 2.5.1+cu121, CUDA 12.1\n- Maniscope parameters: k = 5, α = 0 . 5\n- Datasets: AorB (50 queries), TREC-COVID (50 queries), NFCorpus (323 queries)\n- Models: Maniscope, HNSW (baseline)",
      "## Results: Latency Across Multiple Runs\n\nTable 9 shows per-query latency measurements across three consecutive runs, demonstrating both coldstart behavior and steady-state performance.",
      "## Quality Metric Reproducibility\n\nAll quality metrics (MRR, NDCG@3) remained perfectly stable across all three runs, validating algorithmic determinism. Representative values from all runs:\n\nTable 9: Latency stability verification (milliseconds per query). Maniscope shows consistent warm-run performance with minimal variance, while HNSW exhibits higher variability.\n\n| Dataset    | Model          |   Cold |   Warm-1 |   Warm-2 |   Avg (Warm) |   Std Dev |\n|------------|----------------|--------|----------|----------|--------------|-----------|\n| AorB       | Maniscope HNSW |   11.7 |      7.4 |      7.2 |          7.3 |       0.1 |\n|            | Maniscope HNSW |   53.7 |     63   |     74.6 |         68.8 |       8.2 |\n| TREC-COVID | Maniscope HNSW |    9.5 |      7.2 |      6.5 |          6.9 |       0.5 |\n| TREC-COVID | Maniscope HNSW |  178.1 |    226.5 |    222   |        224.3 |       3.2 |\n| NFCorpus   | Maniscope HNSW |    9.7 |      6.8 |      7   |          6.9 |       0.1 |\n| NFCorpus   | Maniscope HNSW |  174.5 |    169.7 |    173.3 |        171.5 |       2.5 |\n\n| Dataset    | Model          | MRR           | NDCG@3        |\n|------------|----------------|---------------|---------------|\n| AorB       | Maniscope HNSW | 0.9483 0.9533 | 0.8698 0.8463 |\n| TREC-COVID | Maniscope HNSW | 1.0000 1.0000 | 0.9659 0.9506 |\n| NFCorpus   | Maniscope HNSW | 0.8247 0.8237 | 0.7063 0.6602 |",
      "## Key Findings\n\n1. Warm-up efficiency : Maniscope achieves 24-37% latency reduction from cold to warm runs, stabilizing at sub-8ms per query.\n2. Low variance : Maniscope shows exceptional stability with standard deviations of 0.1-0.5ms across warm runs, demonstrating production-ready reliability.\n3. Speedup consistency : Even on cold runs, Maniscope is 4.6-18.8 × faster than HNSW. On warm runs, speedup increases to 9.4-32.5 × .\n4. Quality-latency trade-off : Maniscope maintains competitive or superior quality metrics while achieving sub-8ms latency in steady state.\n5. Perfect reproducibility : Quality metrics are identical across all runs, confirming deterministic behavior essential for production deployments.",
      "## Comparison: Cold vs Warm Performance\n\nThe warm-up phase demonstrates Maniscope's GPU optimization efficiency:\n\n- AorB : 11.7ms (cold) → 7.3ms (warm avg), 37% improvement\n- TREC-COVID : 9.5ms (cold) → 6.9ms (warm avg), 27% improvement\n- NFCorpus : 9.7ms (cold) → 6.9ms (warm avg), 29% improvement\n\nIn contrast, HNSW shows inconsistent warm-up behavior, with some datasets becoming slower after warm-up (AorB, TREC-COVID), suggesting CPU cache contention or thread scheduling issues.",
      "## Data Availability\n\nAll raw results, including per-query latencies and rankings, are available in the supplementary materials:\n\n- Cold run: k5-summary-hnsw-mani 20260123 234736.csv\n- Warm run-1: k5-summary-hnsw-mani 20260123 235051.csv\n- Warm run-2: k5-summary-hnsw-mani 20260123 235340.csv"
    ],
    "relevance_map": {},
    "query_id": "custom_0",
    "num_docs": 75,
    "metadata": {
      "source": "pdf_import",
      "dataset_name": "maniscope-arxiv",
      "pdf_filename": "maniscope-arxiv-v3.2.pdf",
      "has_figures": false,
      "num_pages": 25
    }
  }
]