# Maniscope Environment Configuration
# Copy this file to .env and fill in your actual API keys

# =============================================================================
# OpenRouter API (for LLM-based reranking)
# =============================================================================
# Get your API key from: https://openrouter.ai/keys
OPENROUTER_API_KEY=<your_openrouter_api_key_here>

# OpenRouter API endpoint (default: https://openrouter.ai/api/v1)
# OPENROUTER_URL=https://openrouter.ai/api/v1

# =============================================================================
# Ollama (for local LLM-based reranking)
# =============================================================================
# Ollama API endpoint (default: http://localhost:11434/v1)
# OLLAMA_URL=http://localhost:11434/v1

# =============================================================================
# HuggingFace (optional - for downloading models)
# =============================================================================
# Only needed if you want to download models from HuggingFace Hub
# HF_TOKEN=your_huggingface_token_here

# =============================================================================
# Cache Configuration (optional)
# =============================================================================
# Directory for persistent embedding cache (default: ~/projects/embedding_cache/maniscope)
# CACHE_DIR=/path/to/your/cache/directory

# =============================================================================
# Notes
# =============================================================================
# - OpenRouter API key is required only if using LLM-based rerankers
# - Ollama is free and runs locally (no API key needed)
# - The app works without any API keys for Maniscope engine evaluation
# - HuggingFace token is optional (only for private models or faster downloads)
